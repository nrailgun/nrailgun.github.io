---
layout: post
mathjax: true
title: "系统论文选读与摘要(2018-2020)"
categories: distributed-systems
date: 2020-10-10 00:00:00
---

# 2014

**Project Adam: Building an Efficient and Scalable Deep Learning Training System**

这篇论文指出不一致性反而能增加 accuracy，估计也是 model 相关的性质：

> Surprisingly, it appears that asynchronous training also improves model accuracy.

这篇论文较早，不同于 p3，weight updates 是无锁的，会导致一些噪声。这个方案后来的论文都没有采用，不知道有没有坑。

文中使用了一个 replicated 容灾的 ps，不过其主从切换会有 downtime。

把 $M \times N$ 的 fully-connect layer 的大 matrix 的 gradient 分解成 $k \times M + k \times N$ 两个向量 。由于 batch size $k$ 通常是个 500～5000 左右的数字，这个方法不见得有效。bde 2 的真实情况是 $k=3000, M=10,000, N=1000$，所以这个方法并无用。 

# 2017

**Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters**

论文指出 tf 32x 机器只能有 20x 提升。

wait-free backpropagation (WFBP)：bp 操作是反向的，communication 操作可以在 bp 已经完成的后部提前开始同步进行。tf iteration step 之间存在 global barrier，此外取参数也是在 fp 过程的，其实可以放在 bp 过程。

hybrid communication：根据 param shape 决定是 ps 或 p2p 传播。不过论文的 p2p 传播是 sfb，而不是常见的 all-reduce。

# 2018

**Ray: A Distributed Framework for Emerging AI Applications**

支持 RL 任务，之前都是需要手写 end-to-end 的。

**TVM: An Automated End-to-End Optimizing Compiler for Deep Learning**

1. 图优化：优化方法的理论看起来并不复杂，甚至有点像我之前小组讨论说的几个点。论文是 2018 的，tf 推出 xla 是 2019，在此之前 tf 的图优化估计只是个空壳。
2. 异构硬件的代码自动生成。使用人工编写的 templates 来定义每个operator的优化搜索空间，compiler 可以自动搜索得到其中最优的参数（如循环粒度大小）。

**Gandiva: Introspective Cluster Scheduling for Deep Learning**

讨论了 gpu scheduling，大的 task 分时，小的 task 塞在同一个 gpu。估算的依据是 nn 各个 step 之间的计算 cost 基本相同，随便跑几个 step 就可以准确估算。

**PRETZEL: Opening the Black Box of Machine Learning Prediction Serving Systems**

粗略看了一下 intro，感觉是一个 whitebox 版的 tf-serving，强调数据预处理与计算的并行。作者坦言对比较深的 nn 效果一般。由于目前工作的关注点并不是预处理，所以没有太关注。

# 2019

**PRIORITY BASED PARAMETER PROPAGATION FOR DISTRIBUTED DNN TRAINING**

方法和 byte scheduler 基本一样，感觉 byte scheduler 可能更多的是 framework 无关实现的贡献。

**PipeDream: Generalized Pipeline Parallelism for DNN Training**

大部份现代 framework 都有类似 wfbp 的机制，不过也无法避免等待 gradient 通信。用了 nccl 依旧很严重。该 paper 的方法就是用 pipeline 来尽量用满计算资源。bp apply gradient 完成之前可以先用 prev version 的 parameters 计算 fp，虽然 last mini-batch 的可能看不到，由于训练速度 lr 较低，论文认为完全不影响收敛。

论文比较有参考价值的一点就是用 dp 来划分 pipeline stage；对于比较沉重的 layer，难以保证 stages 之间 cost 接近的情况，用 data parallel。论文假定了 model 是 sequential 的，这一点其实不一定能成立。

知乎上有关于这篇文章的[吐槽](https://www.zhihu.com/question/336446443)，我的观点也比较类似，不过我仍然认为是相当了不起的工作。

**A Generic Communication Scheduler for Distributed DNN Training Acceleration**

论文提出的 schedule 其实和 *Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters* 提出的 wait-free bp 是非常类似的，就是尽量重合 computation & communication。和我之前某次小组讨论提出的“进一步提高并行”的方法本质其实一样的，思想非常简单，主要的难点其实是大部分的框架都有 iter steps 之间的 global barrier 限制。

分片：

- 由于 tf 的 op 顺序是随机的，如果 back 处某个巨大 gradient 先开始传输，会 block 后续发起的传输 front 的 gradient。

- 不需要等待某个巨大 tensor push 完成，可以同步开始 pull，可以充分利用网络的双工。

虽然 byteps 实际上是改动了 tf，看了下代码感觉一言难尽...

> Among all three frameworks, only TensorFlow requires 13-line code change in its engine, which delays the clean-up of communication channels when we cross the global barrier.

**Parity Models: Erasure-Coded Resilience for Prediction Serving Systems**

首先，为了 0.1% 的失败而导致的 tail latency 去大幅度增加系统复杂度是一个比较值得商榷的事情；其次，失败 case 一旦出现，正确率会下降 1～7%，模型性能降级过于严重；最后，需要业务支持去训练一个额外的模型，比较难合作。总而言之，基本不可能落地。

**TASO: Optimizing Deep Learning Computation with Automated Generation of Graph Substitutions**

tf 的 graph substitution 策略是人工的，已经有数百种。taso 被实现在 tvm 和 tensorrt 中。具体算法不明，可以后续关注下。

# 2020

*Taiji: Managing Global User Traffic for Large-Scale Internet Services at the Edge* 讨论了动态路由，有需要可以看下。

*Semeru: A Memory-Disaggregated Managed Runtime* 单个节点只提供非常非常单个的资源（cpu、内存、gpu），通过高速 rdma 互联。

*Overload Control for µs-scale RPCs with Breakwater* 关于过载拒绝，值得关注。

**Splinter: Bare-Metal Extensions for Multi-Tenant Low-Latency Storage**

允许 client 上传一个 rust 的 subset 来执行（避免多次 rpc round-trip），但是同时可以避免安全问题。

**Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis**

感觉应该发在 dl 的会议，就是一个多 step 逐渐 super-resolution 的神经网络。但是，如果用户带宽都不容乐观，怎么会有强力 gpu realtime super-resolution video 呢？其次，nn 对于 super-resolution 虽然可以 beat 传统方法，但是无论如何仍然是失真的，用户体验真的会好吗？

**Serving DNNs like Clockwork: Performance Predictability from the Bottom Up**

nn 运行代价可预测这点是比较显然的，通常整个软件系统本身才是不可预测的。

**Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads**

和 gandiva 一样都是 scheduler，但是考虑了不同硬件对不同模型的性能区别，尽可能高性价比 schedule。

**PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications**

尽可能低延迟地实现 GPU 类似 CPU 的 task 切换。但是我觉得实际上如果用户请求多，那么专门部署专用的 GPU 也完全合理吧，部门经费肯定能下来吧。要是请求不多，那还不是随便搞。

具体思路也是 pipeline，先加载前方的常数，和 pipedream 很像。

**HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees**

也是 scheduler，重点是类似内存分配尽量保证局部性。PCI-e 比宽带通信开销低很多。

**AntMan: Dynamic Scaling on GPU Clusters for Deep Learning**

阿里云超卖 GPU。代码一般优化过 GPU 打满，应该无用。

**KungFu: Making Training in Distributed Machine Learning Adaptive**

看了下 kungfu 的论文，主要卖点是：允许集群在不重启、load checkpoint 的情况下，在运行过程调参。

疑惑点有 3：

- 动态改变 batch size 和 lr 带来两个问题：
  - 如果出现 nan 如何处理，似乎退化成了 load checkpoint。
  - 即使可以顺利改变 lr，很难保证没有在早期不合适 lr 的引导下陷进次次优。
- 似乎无法搜索初始化策略？
- 实验中 ResNet-50 在 cifar-10 最终获得了 ～90% 的 test accuracy，但是早在 2015 年 kaiming he 的论文里 ResNet-44 正确率就已经是 ～93%？

**Ansor: generating high-performance tensor programs for deep learning**

自动生成 nn 的 op kernel。

**A tensor compiler for unified machine learning prediction serving**

把传统的 ml 模型转成 torch 等框架可运行的 graph，感觉是为了复用 dl 强大框架的 serving 功能。

**retiariii: a deep learning exploratory training framework**

dnn 训练中不停调参数调结构是一个非常蛋疼的事情。通常来说 tweak 过程中这些 model 都是相似的，retiarii 用 mutator 来表征这些轻微的区别。

感觉像是在框架级别提供了一种管理 dnn 模型代码的方法。

# Others

**Adaptive Batching for Replicated Servers**

batching（合批）要么 count based 或者 time based，两者都需要人工设置阈值：

- count based 可能等不到 batch size 个 req，一直阻塞。
- time based 如果阈值太大会导致 latency 过大，如果太小则约等于没有 batching。

time adaptive batching 是指用很小的时间阈值 $\mu$，如果 $\mu$ 时间内有另一个 req 到达，则继续排队，否则 batch 发送。

**Detection of Mutual Inconsistency in Distributed Systems**

介绍了 version vector。微软 winfs 有衍生的两篇相关 paper：

- Concise Version Vectors in WinFS

- P2P Replica Synchronization with Vector Sets

**Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis**

介绍了各种神经网络计算分布式与并行相关的工作。文章指出 model parallel 是一个比较难实用的方法，虽然存在一些理论方法：

- Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks

- Beyond Data and Model Parallelism for Deep Neural Networks

- One Weird Trick for Parallelizing Convolutional Neural Networks

**Local AdaAlter: Communication-efficient stochastic gradient descent with adaptive learning rates**

不同于 local optimizer，改造了 adagrad，只同步少量参数。

**The Deep Learning Compiler: A Comprehensive Survey**

介绍了 tvm & xla 在内的各种 graph optimizing。看了之后感觉 graph optimizing 和 generating 是个巨大的坑。

